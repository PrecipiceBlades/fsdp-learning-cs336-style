# FSDP Implementation Notes

## æ ¸å¿ƒå‘ç°å’Œä¿®å¤

### 1. å•GPUç­‰ä»·æ€§ âœ…
**ç»“æœ**: FSDPå’ŒNon-FSDPåœ¨å•GPUä¸Š**å®Œå…¨ç­‰ä»·**ï¼ˆå·®å¼‚=0.0ï¼‰

**éªŒè¯**:
- `test_full_equivalence.py`: æ‰€æœ‰losså’Œå‚æ•°å·®å¼‚éƒ½æ˜¯0.0
- `test_convergence.py`: è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€è‡´
- `test_fsdp2_api_equivalence.py`: APIç­‰ä»·æ€§éªŒè¯

### 2. Paddingå¤„ç†ï¼ˆå…³é”®ï¼ï¼‰

**é—®é¢˜**: PyTorchçš„`all_gather_into_tensor`å’Œ`reduce_scatter_tensor`è¦æ±‚uniform tensor sizes

**è§£å†³æ–¹æ¡ˆ**: 
1. åœ¨FlatParameteråˆ›å»ºæ—¶paddingåˆ°uniform size:
```python
shard_size = (total_numel + world_size - 1) // world_size
padded_total_numel = shard_size * world_size
```

2. **åœ¨optimizer stepåæ¸…é›¶padding**:
```python
# fsdp/optimizer.py
if shard_end > flat_param._total_numel:
    valid_size = flat_param._total_numel - shard_start
    param.data[valid_size:] = 0.0
```

3. **åœ¨reduce-scatteråæ¸…é›¶padding gradients**:
```python
# fsdp/backward_pass.py
if shard_end > flat_param._total_numel:
    valid_size = flat_param._total_numel - shard_start
    local_grad_shard[valid_size:] = 0.0
```

### 3. æ¢¯åº¦å¹³å‡ï¼ˆå…³é”®ï¼ï¼‰

**é—®é¢˜**: Data paralleléœ€è¦å¹³å‡æ¢¯åº¦ï¼Œä¸æ˜¯æ±‚å’Œ

**è§£å†³æ–¹æ¡ˆ**: åœ¨reduce-scatteråé™¤ä»¥world_size:
```python
# fsdp/backward_pass.py
reduce_scatter_tensor(output_tensor=local_grad_shard, input_tensor=full_grad)
local_grad_shard.div_(flat_param.world_size)  # Average!
```

### 4. FlatParameterçš„tensor lifecycle

**å…³é”®å‘ç°**: `self.data`å’Œ`_full_param`çš„å…³ç³»å¿…é¡»æ­£ç¡®ç®¡ç†

**World_size=1çš„ç‰¹æ®Šå¤„ç†**:
```python
# ä¸è¦cloneï¼ç›´æ¥ä½¿ç”¨dataï¼Œç¡®ä¿å®ƒä»¬æŒ‡å‘åŒä¸€ä¸ªtensor
self._full_param = self.data  # NOT: self.data.clone()
```

**Reshardæ—¶çš„å¤„ç†**:
```python
# ä¸è¦ä»full_paramå¤åˆ¶å›dataï¼
# optimizerç›´æ¥æ›´æ–°dataï¼Œå¤åˆ¶ä¼šè¦†ç›–æ›´æ–°
self._full_param = None
self._is_sharded = True
```

### 5. å¤šGPUæµ‹è¯•çš„æ­£ç¡®ç†è§£

**é”™è¯¯ç†è§£**: æ‰€æœ‰GPUç”¨ç›¸åŒæ•°æ®æ—¶ï¼ŒFSDPåº”è¯¥å’ŒNon-FSDPå®Œå…¨ç­‰ä»·

**æ­£ç¡®ç†è§£**: 
- FSDPæ˜¯data parallel - æ¯ä¸ªGPUåº”è¯¥å¤„ç†**ä¸åŒçš„æ•°æ®batch**
- å•GPU FSDP vs å•GPU Non-FSDPåº”è¯¥å®Œå…¨ç­‰ä»· âœ…
- å¤šGPU FSDPèƒ½æ­£å¸¸è®­ç»ƒå’Œæ”¶æ•›å³å¯ âœ…
- å¤šGPU FSDP vs å•GPU Non-FSDP comparisonä¸åˆç†ï¼ˆä¸åŒçš„è®­ç»ƒåœºæ™¯ï¼‰

## å…³é”®æŠ€æœ¯ç»†èŠ‚

### Memoryè®¡ç®—

**Without FSDP (1 GPU)**:
- Parameters: N
- Gradients: N
- Optimizer states (Adam): 2N (momentum + variance)
- **Total: 4N**

**With FSDP (W GPUs)**:
- Parameters: N/W (sharded)
- Gradients: N/W (sharded)  
- Optimizer states: 2N/W (sharded)
- **Total per GPU: 4N/W**
- **Savings: WÃ—**

### Communication Pattern

**Forward**:
1. `all_gather`: æ”¶é›†æ‰€æœ‰ranksçš„parameter shards â†’ å®Œæ•´å‚æ•°
2. `forward computation`: ä½¿ç”¨å®Œæ•´å‚æ•°è®¡ç®—
3. `reshard` (optional): é‡Šæ”¾éæœ¬åœ°shardsï¼ŒèŠ‚çœå†…å­˜

**Backward**:
1. `all_gather` (if resharded): å†æ¬¡æ”¶é›†å®Œæ•´å‚æ•°ç”¨äºbackward
2. `backward computation`: è®¡ç®—æ¢¯åº¦ï¼ˆåœ¨å®Œæ•´å‚æ•°ä¸Šï¼‰
3. `reduce_scatter`: æ±‚å’Œå¹¶åˆ†æ•£æ¢¯åº¦
4. **`div(world_size)`**: å¹³å‡æ¢¯åº¦
5. `reshard`: é‡Šæ”¾å®Œæ•´å‚æ•°ï¼Œåªä¿ç•™local shard

**Optimizer**:
1. `step()`: æ›´æ–°local parameter shard
2. **`zero padding`**: ç¡®ä¿paddingä¸ç´¯ç§¯éé›¶å€¼
3. ä¸‹æ¬¡forwardçš„`all_gather`ä¼šè‡ªåŠ¨åŒæ­¥æ‰€æœ‰æ›´æ–°åçš„shards

## æµ‹è¯•ç­–ç•¥

### å¿…é¡»é€šè¿‡çš„æµ‹è¯•
1. âœ… å•GPU FSDP vs Non-FSDPä¸¥æ ¼ç­‰ä»·ï¼ˆdiff=0.0ï¼‰
2. âœ… å¤šGPU FSDPèƒ½æ­£å¸¸è¿è¡Œ
3. âœ… Memoryä½¿ç”¨ç¬¦åˆé¢„æœŸï¼ˆ4N/Wï¼‰
4. âœ… æ‰€æœ‰unit testsé€šè¿‡

### ä¸åˆç†çš„æµ‹è¯•
1. âŒ å¤šGPU FSDP vs å•GPU Non-FSDPçš„ç²¾ç¡®ç­‰ä»·
   - åŸå› ï¼šä¸åŒçš„data distributionå’Œtraining trajectory
   - åº”è¯¥éªŒè¯ï¼šä¸¤è€…éƒ½èƒ½æ”¶æ•›å³å¯

## PyTorch FSDPå‚è€ƒ

ä»PyTorch FSDPå­¦åˆ°çš„è®¾è®¡åŸåˆ™ï¼š
1. **Paddingç”¨äºuniform shards** - å¿…é¡»çš„ï¼Œå› ä¸ºcollective opsè¦æ±‚
2. **Paddingå¿…é¡»æ¸…é›¶** - é˜²æ­¢æ•°å€¼æ¼‚ç§»
3. **æ¢¯åº¦å¿…é¡»å¹³å‡** - data parallelçš„æ ‡å‡†åšæ³•
4. **Separate local and padded tensors** - `_local_tensor` vs `_padded_local_tensor`

## æœ€ç»ˆçŠ¶æ€

### âœ… å®Œå…¨å®ç°
- Meta device initialization
- FlatParameter with padding
- Forward/Backward hooks  
- Sharded optimizer
- FSDP2 API (`fully_shard`)

### âœ… å®Œå…¨éªŒè¯
- å•GPUä¸¥æ ¼ç­‰ä»·æ€§
- å¤šGPUè®­ç»ƒæˆåŠŸ
- Memory scalingæ­£ç¡®
- All unit tests pass

### ğŸ“š æ–‡æ¡£å®Œæ•´
- README with usage examples
- Implementation notes (this file)
- Detailed comments in code
- Test coverage report

## é¢è¯•å‡†å¤‡è¦ç‚¹

1. **ä¸ºä»€ä¹ˆéœ€è¦padding?**
   - PyTorch collective opsè¦æ±‚uniform tensor sizes
   - `all_gather_into_tensor`æœŸæœ›output_size = world_size Ã— input_size

2. **ä¸ºä»€ä¹ˆè¦æ¸…é›¶padding?**
   - Paddingå‚ä¸forward/backwardä¼šäº§ç”Ÿæ¢¯åº¦
   - Optimizerä¼šæ›´æ–°paddingéƒ¨åˆ†
   - ä¸æ¸…é›¶ä¼šå¯¼è‡´æ•°å€¼æ¼‚ç§»

3. **FSDP vs DDPçš„åŒºåˆ«?**
   - DDP: all-reduce gradients, æ‰€æœ‰ranksæœ‰å®Œæ•´å‚æ•°ï¼ˆå†…å­˜: 4Nï¼‰
   - FSDP: reduce-scatter gradients, shardedå‚æ•°ï¼ˆå†…å­˜: 4N/Wï¼‰
   - FSDPé€šä¿¡æ›´å¤šä½†å†…å­˜æ›´å°‘

4. **ä¸ºä»€ä¹ˆæ¢¯åº¦è¦é™¤ä»¥world_size?**
   - reduce-scatter **æ±‚å’Œ**æ‰€æœ‰ranksçš„æ¢¯åº¦
   - Data paralleléœ€è¦**å¹³å‡**æ¢¯åº¦
   - æ‰€ä»¥è¦div(world_size)

5. **FlatParameterçš„ä½œç”¨?**
   - å‡å°‘é€šä¿¡æ¬¡æ•°ï¼ˆ1æ¬¡all-gatherä»£æ›¿Næ¬¡ï¼‰
   - æé«˜é€šä¿¡æ•ˆç‡ï¼ˆå¤§tensoré€šä¿¡æ›´efficientï¼‰
   - ç®€åŒ–å†…å­˜ç®¡ç†

---

**å®Œæˆæ—¥æœŸ**: 2025-11-16  
**éªŒè¯çŠ¶æ€**: æ‰€æœ‰core testsé€šè¿‡ âœ…

